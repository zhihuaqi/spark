### Exploration using Spark on interactive analysis

- Using Docker while the base container already install pyspark and Jupyter notebook.
- The first thing a Spark program must do is to create a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext) object, which tells Spark how to access a cluster.



